{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import logging\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.5\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "     ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "     --------- ------------------------------ 0.3/1.4 MB 10.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 0.9/1.4 MB 11.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.4/1.4 MB 11.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: click in c:\\users\\rober\\anaconda3\\lib\\site-packages (from nltk==3.5) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\rober\\anaconda3\\lib\\site-packages (from nltk==3.5) (1.2.0)\n",
      "Requirement already satisfied: regex in c:\\users\\rober\\anaconda3\\lib\\site-packages (from nltk==3.5) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rober\\anaconda3\\lib\\site-packages (from nltk==3.5) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rober\\anaconda3\\lib\\site-packages (from click->nltk==3.5) (0.4.6)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434693 sha256=53af9f6388ff136061ea8b24c410a7b2839e9acc970ac50eabe6bffa6fcdb5cc\n",
      "  Stored in directory: c:\\users\\rober\\appdata\\local\\pip\\cache\\wheels\\2a\\15\\d3\\9d3c11455a8402f6764680d7a19167d667203522cbc07262e8\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.8.1\n",
      "    Uninstalling nltk-3.8.1:\n",
      "      Successfully uninstalled nltk-3.8.1\n",
      "Successfully installed nltk-3.5\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rober\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rober\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import logging\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second attempt\n",
    "\n",
    "def get_data(file_path='all-data.csv', encoding='latin', header=None):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file into a Pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: str, the path to the CSV file.\n",
    "    - encoding: str, the character encoding of the file.\n",
    "    - header: int, row (0-indexed) to use as the header. None if the file has no header row.\n",
    "    \n",
    "    Returns:\n",
    "    - A Pandas DataFrame containing the data from the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(file_path, encoding=encoding, header=header)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def split_dataset(dataframe, test_size=0.2, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training, validation, and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: The data frame with all data\n",
    "    - test_size: Proportion of the dataset to include in the test split.\n",
    "    - val_size: Proportion of the training dataset to include in the validation split.\n",
    "    \n",
    "    Returns:\n",
    "    - X_train, X_val, X_test, Y_train, Y_val, Y_test: Split datasets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Assuming the last column is the label\n",
    "        X = dataframe.iloc[:, 0]  # This is your sentiment labels or whatever the first column represents\n",
    "        y = dataframe.iloc[:, 1].astype(str)  # Ensuring text data is treated as strings\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "        val_size_adjusted = val_size / (1 - test_size)  \n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size_adjusted, random_state=42)\n",
    "        logging.info(\"Dataset split into train, validation, and test sets.\")\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to split dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def tokenize_text(texts, max_words=10000, max_len=100):\n",
    "    \"\"\"\n",
    "    Tokenizes text data.\n",
    "    \n",
    "    Parameters:\n",
    "    - texts: List of texts to tokenize.\n",
    "    - max_words: Maximum number of words to consider in the tokenizer vocabulary.\n",
    "    - max_len: Maximum length of the tokenized sequences.\n",
    "    \n",
    "    Returns:\n",
    "    - sequences_padded: Padded sequences of tokenized text.\n",
    "    - tokenizer: The fitted Tokenizer instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    '''\n",
    "    Tokenization converts each unique word in your text dataset into a unique integer (token).\n",
    "    For example, if the word \"the\" is the first word encountered during tokenization, \n",
    "    it might be assigned the token 1. Each other unique word is assigned a subsequent integer value. \n",
    "    The mapping of words to integers depends on the frequency of each word in the dataset, \n",
    "    with the most frequent words getting the lowest integers starting from 1.\n",
    "    '''\n",
    "        \n",
    "    \n",
    "    try:\n",
    "            tokenizer = Tokenizer(num_words=max_words)\n",
    "            tokenizer.fit_on_texts(texts)\n",
    "            sequences = tokenizer.texts_to_sequences(texts)\n",
    "            sequences_padded = pad_sequences(sequences, maxlen=max_len)\n",
    "            logging.info(\"Text tokenization and padding complete.\")\n",
    "            return sequences_padded, tokenizer\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to tokenize text: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def setup_logging(level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Sets up basic logging configuration.\n",
    "    \n",
    "    Parameters:\n",
    "    - level: Logging level, e.g., logging.INFO.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=level)\n",
    "    logging.info(\"Logging setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 12:12:19,721 - INFO - Logging setup complete.\n",
      "2024-02-01 12:12:19,730 - INFO - Dataset split into train, validation, and test sets.\n",
      "2024-02-01 12:12:19,750 - INFO - Text tokenization and padding complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 3],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 2],\n",
       "       [0, 0, 0, ..., 0, 0, 2],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_logging()\n",
    "data = get_data()\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(data)\n",
    "X_train_tokenized, tokenizer = tokenize_text(X_train, max_words=10000, max_len=100)\n",
    "\n",
    "\n",
    "X_train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions to ponder\n",
    "# 1. Why use keras tokenization and why use NLTK\n",
    "# 2. Which steps are needed for which types of NLP? eg, sentiment analysis. \n",
    "# 3. In which cases should you use word vs sentence tokenization?\n",
    "\n",
    "# Next steps (Determine which we need)\n",
    "# 1. Stop words\n",
    "# 2. Stemming\n",
    "# 3. Pos\n",
    "# 4. Lemmatization\n",
    "# 5. Maybe NER for fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
